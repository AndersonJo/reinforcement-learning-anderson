{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Policy Gradients with a baseline\n",
    "\n",
    "\n",
    "## The problem of the PG\n",
    "\n",
    "Policy Gradient는 다음과 같습니다. (REINFORCE Method 참고)\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\theta) =  \\sum^{T}_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t | s_t) \\cdot R_t $$\n",
    "\n",
    "1. PG는 게임이 끝날때까지 기다린 다음에 모델을 학습 가능 -> 학습 속도 저하\n",
    "2. Cumulative reward를 사용하기 때문에 0이 될 수도 있음 -> Good actions 그리고 Bad actions 이 서로 상쇄해서 0값 -> 학습 안됨 \n",
    "3. Cumulative reward를 사용하기 때문에 특정 bad actions 또는 good actions을 학습하지 못함 (good actions이 많은 경우 bad actions을 압도함)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Reducing variance\n",
    "\n",
    "결론적으로 $ R_t $ 가 너무 큰 값이 들어오거나 작은 값이 들어오거나, stable하지 않아서 생기는 문제를 해결하는 방법으로 <br>\n",
    "cumulative reward를 baseline으로 빼줌으로서 조금더 reward를 stable하게 만들어 줍니다.\n",
    "\n",
    "$$ \\nabla_{\\theta} J(\\theta) =  \\sum^{T}_{t=1} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t | s_t) \\cdot (R_t - b(s_t)) $$\n",
    "\n",
    "직관적으로 보면 $ R_t - b(s_t) $ 를 해줌으로서 reward자체가 작아지게 되고, 당연히 gradient값도 작아지게 될 것입니다.<br>\n",
    "예를 들어서 다음과 같다고 할때..\n",
    "\n",
    "$$ \\begin{align} \n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) &= [0.5, 0.2, 0.3] \\\\ \n",
    "R(t) &= [1000, 1001, 1002] \\\\ \n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\cdot R(t) &= [0.5 \\cdot 1000, 0.2 \\cdot 1001, 0.3 \\cdot 1002] \n",
    "\\end{align} $$\n",
    "\n",
    "해당값의 `np.var([0.5*1000, 0.2*1001, 0.3*1002]) = 15524.5` 이 나오게 됩니다. <br>\n",
    "즉 상당히 큰 값의 variance값이 나오게 됩니다.\n",
    "\n",
    "만약 baseline으로 $ R_t $ 에다가 1000을 빼주게 되면 다음과 같이 되게 될 것입니다.\n",
    "\n",
    "$$ \\begin{align} \n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) &= [0.5, 0.2, 0.3] \\\\ \n",
    "R(t) &= [0, 1, 2] \\\\ \n",
    "\\nabla_{\\theta} \\log \\pi_{\\theta}(a_t | s_t) \\cdot R(t) &= [0.5 \\cdot 0, 0.2 \\cdot 1, 0.3 \\cdot 2] \\\\\n",
    "&= 23286.8\n",
    "\\end{align} $$\n",
    "\n",
    "`np.var([0.5 * 0, 0.2*1, 0.3*2]) = 0.062` 값이 나옵니다.<br>\n",
    "즉 작은 gradient값이 나오게 되고 stable한 학습이 가능해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Baseline Functions\n",
    "\n",
    "결론적으로 baseline이라는 것은 기본적으로 reward에 대한 기대값을 빼줌으로서 variance값을 낮추는 것이 목표입니다.\n",
    "\n",
    "$$ \\begin{align} \\nabla_{\\theta} J(\\theta) &= \\nabla_{\\theta} \\log \\pi_{\\theta} (s, a) \\cdot R_t & \\text{REINFORCE} \\\\ \n",
    "&= \\nabla_{\\theta} \\log \\pi_{\\theta} (s, a) \\cdot Q_{w} (s, a) & \\text{Q Actor-Critic} \\\\\n",
    "&= \\nabla_{\\theta} \\log \\pi_{\\theta} (s, a) \\cdot A_{w} (s, a) & \\text{Advantage Actor-Critic} \\\\\n",
    "&= \\nabla_{\\theta} \\log \\pi_{\\theta} (s, a) \\cdot \\delta & \\text{TD Actor-Critic} \\\\\n",
    "\\end{align}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic Method\n",
    "\n",
    "## A2C Forward\n",
    "\n",
    "Advantage Actor Critic Method는 다음과 같습니다.\n",
    "\n",
    "$$ \\begin{align} \\nabla_{\\theta} J(\\theta) &\\sim \\sum^{T-1}_{t=0} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t | s_t) \\left(r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t) \\right)  \\\\\n",
    "&= \\sum^{T-1}_{t=0} \\nabla_{\\theta} \\log \\pi_{\\theta} (a_t | s_t) A(s_t, a_t)\n",
    "\\end{align} $$\n",
    "\n",
    "* $ A(s_t, a_t) $ : Advantage Function\n",
    "* Policy Model: actor model은 현재 state를 보고 **그 다음 action** $ a_{t+1} $ 을 예측합니다.\n",
    "* Value Model: value model은 현재 state를 보고 **그 다음 value** $ r_{t+1} $ 를 예측 합니다.\n",
    "* $ r_{t+1}  = R(s, a) $ : 즉 model(state)를 해서 나온 value값 입니다.\n",
    "* $ \\left(r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t) \\right) $ : TD Error\n",
    "\n",
    "위의 공식에 대한 설명은 다음과 같습니다.<br>\n",
    "아래는 **Advantage Function** 입니다.\n",
    "\n",
    "$$ A(s_t, a_t) = Q_w(s_t, a_t) - V_v (s_t) $$\n",
    "\n",
    "\n",
    "* $ Q_w(s_t, a_t) $ : Q value for an action a in state s\n",
    "* $ V(s) $ : **average value of that state**\n",
    "\n",
    "V function을 baseline function으로 사용함으로서, Q value - V value 를 하게 됩니다. <br>\n",
    "즉, **특정 action을 취하는 것이 일반적으로 기대할수있는 값보다 얼마나 더 좋을가를 나타내는 것입니다**.  <br>\n",
    "해당 action이 해당 state에서 평균적인 action보다 얼마나 더 좋냐? 라고 물어보는 것과도 같습니다.\n",
    "\n",
    "Q value 그리고 V value를 각각 따로 뉴럴 네트워크를 만들필요는 없습니다. (policy network까지 더 추가해서..)<br>\n",
    "Bellmal optimality equation을 사용해서 효율적으로 만들 수 있습니다.\n",
    "\n",
    "$$ Q(s_t, a_t) = \\mathbb{E} \\left[ r_{t+1} + \\gamma V(s_{t+1}) \\right] $$\n",
    "\n",
    "따라서 1-step Advantage 는 다음과 같이 정의할 수 있습니다. \n",
    "\n",
    "$$  A(s_t, a_t) = r_{t+1} + \\gamma V_v(s_{t+1}) - V_v(s_t) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model\n",
    "\n",
    "Policy Update 는 다음과 같이 합니다.\n",
    "\n",
    "$$ \\partial\\theta_{\\pi} = \\partial\\theta_{\\pi} + \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i) (R -V_{\\theta}(s_i)) $$ \n",
    "\n",
    "Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference\n",
    "\n",
    "## TD(1)\n",
    "\n",
    "궁극적으로 Monte Carlo, REINFORCE와 동일하게 episode가 끝날때까지 기다린다음에 update가 가능합니다.\n",
    "\n",
    "$$ \\begin{align} \n",
    "V(s_t) &= V(s_t) + \\alpha(G_t - V(s_t)) \\\\\n",
    "G_t &=  r_{t+1} + \\gamma r_{t+2} + \\gamma r_{t+3} + ... + \\gamma^{T-1}r_T\n",
    "\\end{align} $$\n",
    "\n",
    "* $ T $ : Terminal \n",
    "\n",
    "## TD(0)\n",
    "\n",
    "TD(1)에서는 episode가 끝날때까지의 sum of discounted rewards $ G_t $ 를 사용했었습니다. <br>\n",
    "TD(0)의 경우는 바로 앞단계 1-step reward $ r_{t+1} $ 만 보게 됩니다.\n",
    "\n",
    "$$ V(s_t) = V(s_t) + \\alpha(r_{t+1} + \\gamma V(s_{t+1}) - V(s_t) ) $$\n",
    "\n",
    "## TD(λ) :: Semi Gradient TD\n",
    "\n",
    "TD(0) 와 TD(1) 의 장단점을 합쳐놓은 것이 TD(λ) 입니다. <br>\n",
    "N-Step Bootstrapping이라고도 하며 N 개의 rewards를 사용해서 업데이터를 합니다. <br>\n",
    "문제는 여러개의 rewards가 있는데 어떻게 credit assignment를 할 것인지 입니다. <br>\n",
    "여기에서 사용하는 방법이 Eligibility Traces (ET) 라고 하며,<br>\n",
    "기본적인 방법은 **Recency** 그리고 **Frequency** 에 따라서 credit assignment를 합니다.\n",
    "\n",
    "\n",
    "$$ z_0 = 0 $$\n",
    "$$ E_t(s) = \\gamma \\lambda E_{t-1}(s) + 1 (s_t = s) $$\n",
    "\n",
    "\n",
    "알고리즘은 다음과 같습니다. \n",
    "\n",
    "\n",
    "Loop for each episode: \n",
    "    Initialize S\n",
    "    z = 0\n",
    "    Loop for each step of episode:\n",
    "        Choose $ A $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Step Boostrapping\n",
    "\n",
    "## Background (MC and TD)\n",
    "\n",
    "Monte Carlo (MC)의 경우 에피소드가 끝날때가지 기다려야 하고, <br>\n",
    "One-step temporal difference (TD)의 경우는 다로 다음 스텝까지만 기다리면 됩니다. \n",
    "\n",
    "$$ \\begin{align} \n",
    "\\text{MC | } TD(1) &= V(S_t) = V(S_t) + \\alpha \\left[ G_t - V(S_t) \\right]  \\\\\n",
    " TD(0) &= V(S_t) = V(S_t) + \\alpha \\left[ R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right]\n",
    "\\end{align} $$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. Monte Carlo\n",
    "  * 장점: 특정 state에 bias가 줄어든다 \n",
    "  * 단점: high variance 문제를 겪을 수 있으며, 에피소드가 끝날때 까지 기다려야 하기 때문에 느리다\n",
    "2. Temporal Difference \n",
    "  * 장점: low variance 이며, 다음 스텝 까지만 보기 때문에 효율적이다 \n",
    "  * 단점: 최초에 값이 매우 부정확하거나, 특정 state에 대한 bias가 심하다\n",
    "\n",
    "## N-Step Bootstrapping\n",
    "\n",
    "MC 와 TD의 장점을 서로 합친 개념입니다.\n",
    "\n",
    "Q function은 대략 다음과 같습니다.\n",
    "\n",
    "$$ \\begin{align}\n",
    "Q_{\\pi} &= \\mathbb{E} \\left[ r_0 + \\gamma r_1 + \\gamma^2 r_2 + ... + \\gamma^T r_T \\right] & \\text{Monte Calro} \\\\\n",
    "&= \\mathbb{E} \\left[r_0 + \\gamma V_{\\pi}(s_1) \\right] & \\text{1-step TD} \\\\\n",
    "&= \\mathbb{E} \\left[r_0 + \\gamma r_1 +  \\gamma^2 V_{\\pi}(s_2) \\right] & \\text{2-step TD} \\\\ \n",
    "&= \\mathbb{E} \\left[r_0 + \\gamma r_1 + \\gamma^2 r_2 + \\gamma^3 r_3 + ... + \\gamma^n V_{\\pi}(s_n) \\right] & \\text{n-step TD} \\\\\n",
    "\\end{align} $$\n",
    "\n",
    "**Monte Carlo**\n",
    "\n",
    "$$ G_t = =  R_{t+1} + \\gamma R_{t+2} + \\gamma^2  R_{t+3} + ... + \\gamma^{T-t-1}R_T $$\n",
    "\n",
    "**TD**\n",
    "\n",
    "$$ G_{t:t+1} = = R_{t+1} + \\gamma V_t(S_{t+1}) $$\n",
    "\n",
    "**2-Step return**\n",
    "\n",
    "$$ G_{t:t+1} = = R_{t+1} +  \\gamma R_{t+2} + \\gamma^2 V_t(S_{t+2}) $$\n",
    "\n",
    "**n-step return**\n",
    "\n",
    "$$ G_{t:t+n} = = R_{t+1} +  \\gamma R_{t+2} + ... +  \\gamma^{n-1} R_{t+n} + \\gamma^n V_{t+n-1}(S_{t+n}) $$\n",
    "\n",
    "\n",
    "![](images/actor-critic-n-step.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step Implementation\n",
    "\n",
    "N-Step 의 이슈중에 하나는 $ t $ 라는 현재시점 $ s_t $ 에서  $ a_t $ 을 취했을때 나오는 <br>\n",
    "$ \\text{next_reward} = r_{t+1} $ 그리고 $ \\text{next_state} s_{t+1} $ 밖에 모르는 상황입니다. \n",
    "\n",
    "쉽게 코드로 설명하면 다음과 같습니다. \n",
    "\n",
    "```\n",
    "next_state, next_reward, done, info = env.step(action_t)\n",
    "```\n",
    "\n",
    "문제는 N-Step을 구현하기 위해서는 미래시점의 rewards 들이 필요합니다. <br>\n",
    "즉 이런 것들.. $ r_{t+2}, r_{t+3}, ..., r_{t+n}, s_{t+n} $ \n",
    "\n",
    "따라서 구현상에서는 **time windows of size n 의 크기로 states, actions, 그리고 rewards들을 저장**해놓습니다.<br>\n",
    "그리고 학습시에는 $ V_{\\pi} (S_{t-n}) $ 즉 t-n 시점으로 돌아가서 학습을 할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Advantage Estimation (GAE)\n",
    "\n",
    "## Infinite Geometric Series Formula\n",
    "\n",
    "[칸 아카데미의 Infinite Geometric Series Formula Intuition](https://www.khanacademy.org/math/ap-calculus-bc/bc-series-new/bc-series-optional/v/deriving-geometric-series-sum-formula) 을 참고 합니다.\n",
    "\n",
    "아래의 공식은 GAE 수식 도출에 참고해야 합니다.\n",
    "\n",
    "\n",
    "$$ \\begin{align} \\sum^\\infty_{k=0} = S_{\\infty} &= \\alpha r^0 + \\alpha r^1 + \\alpha r^2 + \\alpha r^3 + ... + \\alpha r^\\infty \\\\\n",
    "r S_{\\infty} &= \\alpha r^1 + \\alpha r^2 + \\alpha r^3 + \\alpha r^4 + ... + \\alpha r^\\infty \\\\\n",
    "S_{\\infty} - r S_{\\infty} &= \\alpha \\cdot 1 \\\\\n",
    "S_{\\infty} (1-r) &= \\alpha \\\\\n",
    "S_{\\infty} &= \\frac{\\alpha}{1-r}\n",
    "\\end{align} $$\n",
    " \n",
    "아래 GAE에서 다시 설명하지만 $ (1-\\alpha) $ 를 TD에다가 곱하게 됩니다. <br>\n",
    "이경우 아래와 같이 모두 합한 값이 1이 되게 만들려고 하는 의도 입니다.\n",
    "\n",
    "$$ (1-r) \\cdot \\frac{\\alpha}{1-r} = \\alpha $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum method   : 19.999999999999982\n",
      "approximation: 19.999999999999982\n"
     ]
    }
   ],
   "source": [
    "lambda_ = 0.95\n",
    "\n",
    "print('sum method   :', sum([lambda_**i for i in range(1000)]))\n",
    "print('approximation:', 1 / (1-lambda_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Advantage Estimation ( $ \\lambda - \\text{return} $ )\n",
    "\n",
    "Monte carlo의 경우 episode가 끝날때까지 기다려야 하기 때문에 학습 속도에 있어서 문제가 있으며, <br>\n",
    "1-step temporal difference learning의 경우 속도는 빠르지만 bias한 것이 문제가 될 수 있습니다.<br>\n",
    "그래서 N-Step Learning이 나오게 되었지만, 역시 여전히 최적의 n값을 지정하는 것은 다시 문제가 될 수 있습니다. \n",
    "\n",
    "$ TD(\\lambda) $ 는 이러한 문제를 해결하기 위해서 나온 알고리즘입니다. <br>\n",
    "기본적인 아이디어는 단 하나의 최적의 n값을 사용하는것이 아니라 모든 가능한 n-step TD의 weighted sum을 적용하는 것입니다. \n",
    "\n",
    "먼저 advantage function을 정의하면 다음과 같습니다. <br>\n",
    "\n",
    "$$ \\text{advantage function} = A(s_t, a_t) = r_t + \\gamma V(s_{t+1}) - V(s_t)  $$\n",
    "\n",
    "\n",
    "n-step에 따라서 advantage function의 모습은 다음과 같이 변합니다.\n",
    "\n",
    "\n",
    "$$ \\begin{align} \n",
    "& A^{(1)}_t = \\delta_t &  &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "& A^{(2)}_t = \\delta_t + \\gamma \\delta_{t+1} &  &= r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+2}) - V(s_t) \\\\\n",
    "& A^{(3)}_t = \\delta_t + \\gamma \\delta_{t+1} + \\gamma ^2 \\delta_{t+2} &  &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V(s_{t+3}) - V(s_t)\n",
    "\\end{align} $$\n",
    "\n",
    "\n",
    "최종적으로 Generalized Advantage Estimation (GAE) 는 exponentially-weighted average of n-step estimators 로 정의가 될 수 있습니다. <br>\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A_t^{GAE(\\gamma,\\lambda)} &= (1-\\lambda)\\Big(A_{t}^{(1)} + \\lambda A_{t}^{(2)} + \\lambda^2 A_{t}^{(3)} + \\cdots \\Big) \\\\\n",
    "&= (1-\\lambda)\\Big(\\delta_t^V + \\lambda(\\delta_t^V + \\gamma \\delta_{t+1}^V) + \\lambda^2(\\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V)+ \\cdots \\Big)  \\\\\n",
    "&= (1-\\lambda)\\Big( \\delta_t^V(1+\\lambda+\\lambda^2+\\cdots) + \\gamma\\delta_{t+1}^V(\\lambda+\\lambda^2+\\cdots) + \\cdots \\Big) \\\\\n",
    "&= (1-\\lambda)\\left(\\delta_t^V \\frac{1}{1-\\lambda} + \\gamma \\delta_{t+1}^V\\frac{\\lambda}{1-\\lambda} + \\cdots\\right) \\\\\n",
    "&= \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^{V}\n",
    "\\end{align} $$\n",
    "\n",
    "\n",
    "* $ \\lambda $ : 0 ~ 1 사이의 값\n",
    "* $ GAE(\\gamma, 0) $ : $ A_t = \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) $\n",
    "* $ GAE(\\gamma, 1) $ : $ A_t = \\sum^\\infty_{j=0} \\gamma^j r_{t+j} - V(s_t) $\n",
    "\n",
    "즉 $ \\lambda $ 값을  0로 맞추면 1-step TD가 되고, 1로 맞추면 Monte Carlo가 되며, <br>\n",
    "이 두가지를 서로 섞었다고 볼 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "$$ G^{\\lambda}_t = (1-\\lambda) \\sum^{\\infty}_{i=1} \\lambda^{i-1} G_{t:t+i} $$\n",
    "\n",
    "뭐 이론적으로는 이런데.. 제가 생각하는 부분은 normalization부분이 크고, <br>\n",
    "아주 쉽게 생각하면 그냥 gradient를 엄청나게 작게 줄여준다가 핵심일듯 합니다. \n",
    "\n",
    "$ (1 - \\lambda) $ 를 곱하게 되는데, normalization과 같은 효과가 있습니다. <br>\n",
    "만약 $ \\lambda = 0.99 $ 라면 1 - 0.99 = 0.01 이 되기 때문에 쉽게 생각하면 gradient값이 상당하게 줄어듭니다. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5ef885da90>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "s = 0\n",
    "data = []\n",
    "for i in range(1000):\n",
    "    s += (1 - gamma) * gamma**i\n",
    "    data.append(s)\n",
    "\n",
    "sns.lineplot(range(1000), data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@violante.andre/simple-reinforcement-learning-temporal-difference-learning-e883ea0d65b0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1.0,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
